{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5: Machine Translation\n",
    "\n",
    "Machine translation is an application of NLP to computationally translate text from one language to the other. It is one of the most popular fields of research in the NLP community. Very recently (2014) deep learning methods changed the face of machine translation and all of NLP. The application of deep learning to machine translation was referred to as Neural Machine Translation (NMT).\n",
    "\n",
    "In this mini lab, we will see a sample code that performs neural machine translation, and we will see the effects of various hyperparameters on the performance of the system.\n",
    "\n",
    "For simplicity, we will take as our dataset, a set of short English sentences mapped to French sentences, and instead of performing the translation one word at a time, it will be performing it one character at a time.\n",
    "\n",
    "Total points: 20 points + 10 bonus points\n",
    "\n",
    "**Submission Instructions**: Just upload this notebook, with all your answers in the respective cells, to Compass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code contains the full code of the functional NMT system. Run it to get a sense of how the loss and accuracy of the training data and the validation data are changing with every epoch. No data is necessary to download; all is included in the lab's directory.\n",
    "\n",
    "#### Packages to install:\n",
    "- pip install keras\n",
    "- pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 12s 1ms/sample - loss: 1.2342 - accuracy: 0.7237 - val_loss: 1.0876 - val_accuracy: 0.6983\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 8s 947us/sample - loss: 0.9081 - accuracy: 0.7491 - val_loss: 0.9700 - val_accuracy: 0.7352\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 8s 957us/sample - loss: 0.7858 - accuracy: 0.7862 - val_loss: 0.8415 - val_accuracy: 0.7682\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 8s 963us/sample - loss: 0.6843 - accuracy: 0.8079 - val_loss: 0.7555 - val_accuracy: 0.7863\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 8s 972us/sample - loss: 0.6221 - accuracy: 0.8218 - val_loss: 0.7023 - val_accuracy: 0.7959\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 8s 966us/sample - loss: 0.5838 - accuracy: 0.8313 - val_loss: 0.6694 - val_accuracy: 0.8045\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 8s 980us/sample - loss: 0.5567 - accuracy: 0.8382 - val_loss: 0.6382 - val_accuracy: 0.8131\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 8s 959us/sample - loss: 0.5350 - accuracy: 0.8439 - val_loss: 0.6189 - val_accuracy: 0.8183\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 8s 974us/sample - loss: 0.5168 - accuracy: 0.8489 - val_loss: 0.6023 - val_accuracy: 0.8233\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 8s 983us/sample - loss: 0.5011 - accuracy: 0.8536 - val_loss: 0.5836 - val_accuracy: 0.8285\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her swim.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him jump.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him once.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it first.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it on TV.\n",
      "Decoded sentence: J'ai de le maine.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n",
      "-\n",
      "Input sentence: I saw the cook.\n",
      "Decoded sentence: J'ai de le partie.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "**Summary of the algorithm**\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "**Data download**\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(8000,8010):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training an NMT system (or any deep learning system), one of the decisions to take is the batch size: number of examples processed (in parallel) before updating parameters.\n",
    "\n",
    "**Deliverable 1** (5 points): Change the batch size from 64 to 128, and then 256. How did it affect the speed of training (time per epoch)? Is it slower or faster? Why? What was its effect on the increase of accuracy from one epoch to the other? What is the concluded tradeoff between a smaller batch size and a larger one?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#When the batch size was 64 the first epoch took about 12s and the rest took about 8s and highest accuracy was about 0.8552. With bathch size 128 the first epoch took 12s anf the 2nd and 3rd took 9s after which the rest took 8s and highest accuracy was 0.8306. With batch size 256 the first epoch took 12s and the rest took about 7s and the highest accuracy was 0.7943. I observed that smaller batch sizes took medium amount of time and performed with highest accuracy. As we increase Batch size, we slightly increase the training time while slightly reducing accuracy after a point where a larger batch size decreases training time and decreases accuracy. I conclude that small batch sizes about 64 are the best as they take a medium amount of time and provide highly accurate results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another decision to make about the architecture of the NMT is the latent dimension size of the LSTM block. The LSTM block encodes semantics of a sentence to a vector of size equal to the latent dimension. This controls the modeling capacity of our system.\n",
    "\n",
    "**Deliverable 2** (5 points): Change the latent dimension from a 100 to a 10, and a 1000. How did it affect speed of training? Why? How did affect the accuracy of the system?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#When the latent dimension was 10 the first epoch took about 7s and the rest took 4s the highest accuracy was about 0.7703. When the latent dimension was 100 the first epoch took about 12s and the rest took about 8s and highest accuracy was about 0.8552. When the latent dimension was 1000 all epochs took about 175s and highest accuracy was about 0.9164.\n",
    "I conclude that as latent dimension size is increased, the accuracy is increased but the training time per epoch increases at a larger rate. Thus, there is an optimum latent dimension (100 for this case) that provides a decently high accuracy after consuming a comparably optimal amount of training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3** (5 points): One of the issues with the code above is that at demo time (for loop at the bottom of the code), the test is performed on training instances instead of validation instances. Adjust the code to perform the demo on the first 10 instances of the validation instances, and copy the cell's output below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Input sentence: I saw her home.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her home.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her swim.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him jump.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him once.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it first.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it on TV.\n",
    "Decoded sentence: J'ai de le maine.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: J'ai de le partie.\n",
    "\n",
    "-\n",
    "Input sentence: I saw the cook.\n",
    "Decoded sentence: J'ai de le partie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4** (5 points): In the following piece of code `decoder_dense = Dense(num_decoder_tokens, activation='softmax')` a dense layer is instantiated to map the output of the LSTM block to a prediction of the next character. Why is the output size of this dense layer set to `num_decoder_tokens`, which is the number of possible French output characters?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#The output of this Dense Layer is set to the number of possible French output characters as this layer is predicting the next character. While translating from english to french , the probability distribution of the next character lies amongst the possible french tokens so the output size is set to num_decoder_tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question** (10 points): During training, the model, besides being fed the target data (French translation) as in the normal supervised procedure, it is also fed the target data shifted by one position to the left. What is the purpose of this practice?  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Insert answer here\n",
    "I firstly think It is because the prediction of the next character is dependant on the previous target data. If this is not the case, Read Below - \n",
    "\n",
    "I am not too sure but I believe it has something to do with optimizing the prediction for the next character. It helps verify and provide more accurace predictions for the next character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
